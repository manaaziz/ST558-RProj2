---
title: "ST558 R Project II"
author: "Mana Azizsoltani"
date: "October 16, 2020"
output: 
  rmarkdown::github_document:
    toc: true
    toc_depth: 1
    pandoc_args: --webtex
params:  
    day: "monday"
---

```{r setup, include=FALSE, message=FALSE}
# Set seed for reproducibility
set.seed(445)

# Load necessary libraries
library(knitr)
library(gridExtra)
library(rmarkdown)
library(caret)
library(tidyverse)
library(class)
library(randomForest)
library(gbm)
library(corrplot)
library(klaR)
```  

# Introduction  
## Online News Data Set  
The online news data set that we will be working with for this project was found on the UCI Machine Learning Repository. The data itself comes from the popular online news website, Mashable. Each observation of the data set represents a single article released by Mashable during the period from January 7, 2013 and January 7, 2015. Each of the 39644 articles has 60 attributes that describes the different aspects of each article.  

## Variable Descriptions  
The variables that I will be using are the following (more on selection later):  

1. num_hrefs: Number of links  
2. data_channel_is_entertainment: Is data channel 'Entertainment'?  
3. data_channel_is_socmed: Is data channel 'Social Media'?  
4. data_channel_is_tech: Is data channel 'Tech'?  
5. data_channel_is_world: Is data channel 'World'?  
6. kw_min_min: Worst keyword (min. shares)  
7. kw_min_avg: Avg. keyword (min. shares)  
8. kw_max_avg: Avg. keyword (max. shares)  
9. kw_avg_avg: Avg. keyword (avg. shares)  
10. LDA_00: Closeness to LDA topic 0  
11. LDA_01: Closeness to LDA topic 1  
12. LDA_02: Closeness to LDA topic 2  
13. LDA_04: Closeness to LDA topic 4  
14. global_subjectivity: Text subjectivity  
15. global_sentiment_polarity: Text sentiment polarity  
16. rate_negative_words: Rate of negative words among non-neutral tokens  
17. title_subjectivity: Title subjectivity  
18. shares: Number of shares (target)  

## Purpose and Methods  
The purpose of this analysis is to predict the number of shares of a particular article using tree-based machine learning techniques. In particular, we will be using a normal (non-ensemble) regression tree and a boosted tree model to attempt to predict the number of shares. This process will be done for each day of the week.  

# Data  
## Reading in the Data  
First things first, we need to read in the data and filter the data for `r str_to_title(params$day)`.  
```{r, message=FALSE}
# Read in data set
dat <- read_csv("OnlineNewsPopularity.csv") %>% dplyr::select(-url)

# Subset the data set by day
dayvar <- as.name(paste0("weekday_is_", params$day))
day.dat <- dat %>% filter((!!sym(dayvar)) == 1)
```  

## Variable Selection  
After reading the article attached to the UCI Machine Learning website where we got the data, I decided to use the same technique as Dr. Ren and Dr. Yang as discussed in [their paper](http://cs229.stanford.edu/proj2015/328_report.pdf). They calculated the Fisher score for each feature and selected the 20 with the highest Fisher scores. The Fisher score for the $j^{\text{th}}$ feature is given by:  

$$F(j) =  \frac{(\bar{x}^1_j - \bar{x}^2_j)^2}{(s^1_j)^2 + (s^2_j)^2},$$  
where  
$$(s^k_j)^2 = \sum_{x \in X^k}(x_j - \bar{x}^k_j)^2$$  

The top variables with the highest Fisher scores were the following:  

```{r fscores, echo=FALSE}
knitr::include_graphics("fscores.png")
```   

All that being said, I used 17 of these 20 variables as my predictors in my models, since the `is_day` variables are irrelevant if we are doing the reports by day. This is also consistent with what we learned in the lectures about how many predictors to use in a regression tree, which is one-third of the total number of predictors, or 20 in this case.  

## Partitioning  
Before creating the models, we must split the data into a training and test data set in order to later evaluate the model's prediction accuracy. In this case we will be using a 70/30 split, training the data on the 70% and testing the trained models on the 30%.    
```{r datasplit}
vars <- c(7, 14, 16:19, 25:27, 39:41, 43:45, 49, 56, 60)
index <- createDataPartition(day.dat$shares, p = .7, list = F) %>% as.vector()
train <- day.dat[index,vars]
test <- day.dat[-index,vars]
```  

# Summarization  
To get an idea of what we are working with when considering this data, we want to get some summaries of the data. Before looking at the training data, I wanted to see how much of the data pertained to each day.  
```{r plot1, echo=FALSE}
counts <- round(colSums(dat[,31:37])/nrow(dat)*100, 1)
DOW <- c("Monday", "Tuesday", "Wednesday", "Thursday",
         "Friday", "Saturday", "Sunday")
df.counts <- cbind.data.frame("Day" = DOW, "Count" = counts)
df.counts$Day <- factor(df.counts$Day, levels = df.counts$Day)

ggplot(data = df.counts, aes(x = Day, y = counts)) + geom_bar(stat = "identity", fill = "#CC0000") + 
  labs(x = "Day of Week", y = "Percentage", title = "Percentage of Data for Each Day") + 
  scale_y_continuous(limits = c(0, 22)) + geom_text(aes(label=counts), vjust = 1.6, color = "white", size = 4)

```  

Next, I went ahead and looked at summaries of the four variables with the highest Fisher Scores. The case of the quantitative variables, I plotted them on a scatter plot against `shares`, and for the qualitative variables, I just whipped up a bar plot to get a breakdown of the training data.
```{r sctrs, message=FALSE, echo=FALSE, warning=FALSE}
train2 <- train %>% filter(shares < max(shares))

gg1 <- ggplot(data = train2, aes(x = (kw_avg_avg-mean(kw_avg_avg))/sd(kw_avg_avg),
                                 y = (shares-mean(shares))/sd(shares))) + geom_point() +
         labs(y = "Shares (Standardized)", x = "Avg. Keyword (Standardized)", title = "Shares vs. Avg. Keyword")

gg2 <- ggplot(data = train2, aes(x = LDA_02, y = shares)) + geom_point() + 
         labs(x = "Closeness to LDA topic 2", y = "Shares", title = "Shares vs. LDA_02")

gg3 <- ggplot(data = train, aes(x = data_channel_is_world)) + geom_bar() + 
         scale_x_discrete(limits = c(0,1), labels = c("Not World Channel", "World Channel")) + 
         labs(x = "Is data channel 'World'?", y = "Count", title = "Is World Channel Breakdown")

gg4 <- ggplot(data = train, aes(x = data_channel_is_socmed)) + geom_bar() + 
         scale_x_discrete(limits = c(0,1), labels = c("Not S.M. Channel", "S.M. Channel")) + 
         labs(x = "Is data channel 'Social Media'?", y = "Count", title = "Is Social Media Channel Breakdown")

grid.arrange(gg1, gg2, gg3, gg4, nrow = 2)
```  

Then I looked at the numeric summaries of some of the quantitative variables as well as contingency tables of the qualitative variables. I wanted to get an idea of the range and distribution of the variables.  
```{r, echo=FALSE}
summary(train[,c(1, 9, 12, 14, 15)])
```  

```{r, echo=FALSE}
df2 <- apply(train[,c(2:5)], MARGIN = 2, FUN = "table")
colnames(df2) <- c("is ent.", "is soc. med.", "is tech", "is world")
rownames(df2) <- c("No", "Yes")
kable(df2)
```  

# Modeling  
As mentioned above, I will be running a basic, non-ensemble regression tree as well as a boosted tree in order to try and predict the number of shares of a particular article. For both models, I used the `caret` package in R to all the heavy lifting of cross-validation and tuning for me.  

## Regression Tree  
Firstly, I went ahead and fit the basic, non-ensemble regression tree model on the training data. The typical regression tree is split using recursive binary splitting, meaning that for every possible value of each predictor, it finds the SSR and tries to minimize it. After growing a large tree (many splits), the tree is pruned back using cost-complexity pruning, which is done to prevent overfitting the data. I chose the Cp value (tuning parameter for pruning) using the default tune grid from the `train()` function in `caret`. For this model, I used Leave-One-Out Cross Validation (LOOCV) as instructed in the project outline. I didn't bother scaling the data since for a basic regression tree it doesn't really make a difference. After running the model, I used it to predict the observations in the test data set and calculate the Root Mean Square Error (RMSE).  
```{r tree}
# Specify CV method
trctrl <- trainControl(method = "LOOCV")

# Normal Regression Tree
treeFit <- train(shares ~., data = train, method = "rpart",
                 trControl=trctrl)
treePred <- predict(treeFit, newdata = test)
treeRMSE <- sqrt(mean((treePred-test$shares)^2))
```  
The final regression tree model that I selected was the model with a Cp value of `r round(unname(treeFit$bestTune[1,1]), 5)`.  

## Boosted Tree
After fitting the basic regression tree, we can go ahead and fit the boosted tree model on the training data. Boosting is an ensemble tree-based model in which the trees are slowly trained to avoid overfitting. The trees are grown sequentially, so that each subsequent tree is grown on a modified version of the original data. The predictions are then updated using the residuals as the trees are grown. I used a 5-fold cross validation to evaluate the boosted tree model. Unlike the previous model, I went ahead and centered and scaled the data in the pre-processing stage. After running the model, I used it to predict the observations in the test data set and calculate the Root Mean Square Error (RMSE).  
```{r boost}
# Boosted Regression Tree
trctrl2 <- trainControl(method = "cv", number = 5)
boostFit <- train(shares ~., data = train, method = "gbm",
                  trControl = trctrl2, preProcess = c("center", "scale"),
                  verbose = FALSE)
boostPred <- predict(boostFit, newdata = test)
boostRMSE <- sqrt(mean((boostPred-test$shares)^2))
```  

The final boosted tree model that I selected was the model with the following tune of the parameters:  
```{r boostbest, echo=FALSE}
boostFit$bestTune
```  

## Linear Regression

Fit a linear regression model. Find predictions on the test set and the RMSE. 

```{r Linear Regression}
linFit <- train(shares ~ ., 
                data = train, 
                method = "lm", 
                preProcess = c("center", "scale")) 

linPred <- predict(linFit, newdata = test)

linRMSE <- sqrt(mean((linPred-test$shares)^2))
```

## Model Comparison  
```{r RMSEtbl, echo=FALSE}
tbl.rmse <- rbind.data.frame("tree" = treeRMSE, "boost" = boostRMSE, "linear regression" = linRMSE)
colnames(tbl.rmse) <- "RMSE"
rownames(tbl.rmse) <- c("Reg. Tree", "Boost Tree", "Linear Regression")
kable(tbl.rmse, caption = "Comparison of Models' RMSE")
```  
After evaluating the predictions of the test data set from each model, the basic regression tree had an RMSE of `r round(tbl.rmse[1,1])` and the boosted tree model had an RMSE of `r round(tbl.rmse[2,1])`. The linear regression model had a RMSE of `r round(tbl.rmse[3,1])`. I expected the boosted tree to have a lower RMSE, since we learned that ensemble trees tend to outperform single trees in terms of prediction.

